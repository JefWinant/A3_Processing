{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf559b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5eabd",
   "metadata": {},
   "source": [
    "# Delete empty folders (no reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf393777",
   "metadata": {},
   "source": [
    "Received data has a bunch of empty folders for timestamps where no new reviews were received. In this section we remove them so later processing will have be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_size(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_small_folders(parent_dir, size_limit):\n",
    "    for root, dirs, files in os.walk(parent_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            folder_size = get_folder_size(dir_path)\n",
    "            if folder_size < size_limit:\n",
    "                print(f\"Deleting folder: {dir_path}\")\n",
    "                shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty folders, with no reviews, will have file size of 8 bytes.\n",
    "path = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Raw_Data\\2023-04-13-20-39\"\n",
    "size_limit = 9  # Specify the size limit in bytes\n",
    "delete_small_folders(path, size_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a661bb7",
   "metadata": {},
   "source": [
    "# Combine Received files into single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6916ce4",
   "metadata": {},
   "source": [
    "Here we take all the individual received text files and combine them into a single, easy to use csv.\n",
    "We also clean the text of the reviews of newlines, so it can easily be converted into csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bff92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting the first dataframe of a folder, so we can concatenate it with the rest\n",
    "def initialize_dataframe(folder_path):\n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        for dirname in dirnames:\n",
    "            subpath = os.path.join(dirpath, dirname)\n",
    "            df = spark.read.json(subpath)\n",
    "            \n",
    "            # Here we remove new lines from the text\n",
    "            df = df.withColumn(\"review_text\",F.translate(\"review_text\",'\\n',' '))\n",
    "            \n",
    "            return df\n",
    "\n",
    "def combine_into_dataframe(folder_path):\n",
    "    df = initialize_dataframe(folder_path)\n",
    "    firstdir = True\n",
    "    \n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        for dirname in dirnames:\n",
    "            # We already have the first review, so skip that one\n",
    "            if(firstdir):\n",
    "                firstdir = False\n",
    "                continue\n",
    "            subpath = os.path.join(dirpath, dirname)\n",
    "            df1 = spark.read.json(subpath)\n",
    "            df1 = df1.withColumn(\"review_text\",F.translate(\"review_text\",'\\n',''))\n",
    "            \n",
    "            df = df.union(df1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all reviews of a folder into a single dataframe\n",
    "\n",
    "df = combine_into_dataframe(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263572e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the reviews in one big beautiful csv\n",
    "folders = os.path.split(path)\n",
    "last_folder = folders[-1]\n",
    "df.coalesce(1).write.option(\"header\",True).csv(\"full_csv_\"+last_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453afa71",
   "metadata": {},
   "source": [
    "# Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-11-12-45\"\n",
    "path2 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-11-22-16\"\n",
    "path3 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-12-11-00\"\n",
    "path4 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-12-15-05\"\n",
    "path5 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-12-20-05\"\n",
    "path6 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-13-13-51\"\n",
    "path7 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-13-17-57\"\n",
    "path8 = r\"C:\\Users\\jef-w\\Desktop\\Uni\\KUL\\Year_1\\Advanced_Analytics\\Assignments\\Assignment_3\\spark\\notebooks\\Processed_Data\\full_csv_2023-04-13-20-39\"\n",
    "\n",
    "df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path1)\n",
    "df2 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path2)\n",
    "df3 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path3)\n",
    "df4 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path4)\n",
    "df5 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path5)\n",
    "df6 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path6)\n",
    "df7 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path7)\n",
    "df8 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path8)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.union(df3)\n",
    "df = df.union(df4)\n",
    "df = df.union(df5)\n",
    "df = df.union(df6)\n",
    "df = df.union(df7)\n",
    "df = df.union(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15277d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the reviews in one big beautiful csv\n",
    "df.coalesce(1).write.option(\"header\",True).csv(\"full_data_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad979a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
